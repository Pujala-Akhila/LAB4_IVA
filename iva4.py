# -*- coding: utf-8 -*-
"""iva4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JQpGDncyztk-GDg2Uhu3dRRKixJqPIBZ
"""

import numpy as np
import cv2
import argparse
import matplotlib.pyplot as plt

def load_video_and_process(video_path, threshold_value=128):
    # Load the video
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Unable to open video.")
        return

    # Lists to store frames and segmented masks
    frames = []
    segmented_frames = []

    # Loop through the video and extract frames
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Convert the frame to grayscale for simpler processing
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Apply a threshold to perform segmentation (Spatio-Temporal Segmentation)
        _, segmented_frame = cv2.threshold(gray_frame, threshold_value, 255, cv2.THRESH_BINARY)

        # Store the original frame and segmented frame
        frames.append(frame)
        segmented_frames.append(segmented_frame)

    cap.release()

    return frames, segmented_frames

def show_frame(frame, title="Frame"):
    """ Function to display a single frame using matplotlib """
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display
    plt.title(title)
    plt.axis('off')
    plt.show()

def show_segmented_frame(segmented_frame, title="Segmented Frame"):
    """ Function to display a segmented frame using matplotlib """
    plt.imshow(segmented_frame, cmap='gray')
    plt.title(title)
    plt.axis('off')
    plt.show()

def process_and_track_objects(frames, segmented_frames):
    # Simple background subtraction to track objects (Foreground vs Background)
    fg_bg_subtractor = cv2.createBackgroundSubtractorMOG2()

    for i in range(len(frames)):
        # Apply background subtraction to each frame to get the foreground mask
        fg_mask = fg_bg_subtractor.apply(frames[i])

        # Show the original frame, segmented frame, and foreground mask
        show_frame(frames[i], title=f"Original Frame {i}")
        show_segmented_frame(segmented_frames[i], title=f"Segmented Frame {i}")
        show_segmented_frame(fg_mask, title=f"Foreground Mask {i}")

# Example usage
video_path = "/content/sample_data/scut - Made with Clipchamp.mp4"

# Step 1: Load the video and extract frames and their segmentation
frames, segmented_frames = load_video_and_process(video_path)

# Step 2: Track objects and analyze motion and changes
process_and_track_objects(frames, segmented_frames)

def load_video_and_process(video_path, threshold_value=128):
    # Load the video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Unable to open video.")
        return
    # Lists to store frames and segmented masks
    frames = []
    segmented_frames = []
    # Loop through the video and extract frames
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        # Convert the frame to grayscale for simpler processing
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Apply a threshold to perform segmentation (Spatio-Temporal Segmentation)
        _, segmented_frame = cv2.threshold(gray_frame, threshold_value, 255, cv2.THRESH_BINARY)
        # Store the original frame and segmented frame
        frames.append(frame)
        segmented_frames.append(segmented_frame)
    cap.release()
    return frames, segmented_frames

def show_frame(frame, title="Frame"):
    """ Function to display a single frame using matplotlib """
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display
    plt.title(title)
    plt.axis('off')
    plt.show()

def show_segmented_frame(segmented_frame, title="Segmented Frame"):
    """ Function to display a segmented frame using matplotlib """
    plt.imshow(segmented_frame, cmap='gray')
    plt.title(title)
    plt.axis('off')
    plt.show()

def process_and_track_objects(frames, segmented_frames):
    # Define the frame ranges for edge detection
    frame_ranges = [(1, 10), (100, 110), (200, 210), (300, 310)]

    for i in range(len(frames)):
        # Check if the current frame index is in any of the defined ranges
        for start, end in frame_ranges:
            if start <= i < end:
                # Perform edge detection on the segmented frame using Canny
                edges = cv2.Canny(segmented_frames[i], 100, 200)

                # Show the original frame, segmented frame, and edge-detected frame
                show_frame(frames[i], title=f"Original Frame {i}")
                show_segmented_frame(segmented_frames[i], title=f"Segmented Frame {i}")
                show_segmented_frame(edges, title=f"Edge Detected Frame {i}")
                break  # Break to avoid redundant checks for other ranges

# Example usage
video_path = "/content/sample_data/scut - Made with Clipchamp.mp4"

# Step 1: Load the video and extract frames and their segmentation
frames, segmented_frames = load_video_and_process(video_path)

# Step 2: Process specified frames and analyze motion and changes using edge detection
process_and_track_objects(frames, segmented_frames)

def extract_frames_and_compare(video_path, bins=50):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Unable to open video.")
        return

    frames = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Convert frame to HSV
        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

        # Split the HSV channels
        h, s, v = cv2.split(hsv_frame)

        # Apply histogram equalization to the V channel
        equalized_v = cv2.equalizeHist(v)

        # Merge back HSV channels
        equalized_hsv = cv2.merge([h, s, equalized_v])

        # Store the frame
        frames.append(equalized_hsv)

    cap.release()

    # Function to calculate the histogram using np.histogram for each channel
    def calculate_histogram(image, bins=bins):
        h_channel, s_channel, v_channel = cv2.split(image)

        # Calculate histograms for H, S, and V channels
        h_hist, _ = np.histogram(h_channel, bins=bins, range=(0, 256))
        s_hist, _ = np.histogram(s_channel, bins=bins, range=(0, 256))
        v_hist, _ = np.histogram(v_channel, bins=bins, range=(0, 256))

        # Normalize the histograms
        h_hist = h_hist / np.sum(h_hist)
        s_hist = s_hist / np.sum(s_hist)
        v_hist = v_hist / np.sum(v_hist)

        # Combine histograms into a single array
        combined_hist = np.concatenate([h_hist, s_hist, v_hist])

        return combined_hist

    # Function to calculate intersection score between two histograms
    def histogram_intersection(hist1, hist2):
        return np.sum(np.minimum(hist1, hist2))

    # Compute histograms for adjacent frames and compare them
    scores = []
    histograms = []
    for frame in frames:
        hist = calculate_histogram(frame)
        histograms.append(hist)

    # Compare adjacent histograms and calculate intersection scores for every consecutive frame
    for i in range(len(histograms) - 1):
        score = histogram_intersection(histograms[i], histograms[i + 1])
        scores.append(score)
        print(f"Intersection score between frame {i} and frame {i + 1}: {score:.4f}")
    # Display some frames using matplotlib
    def show_frame(frame, title="Frame"):
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_HSV2RGB)  # Convert back to RGB for display
        plt.imshow(rgb_frame)
        plt.title(title)
        plt.axis('off')
        plt.show()
    # Identify and display the frames with scores less than the average
    j=k=1
    for i, score in enumerate(scores):
        if score < 2.8:
          if score>2.2:
            print("Scene_Cut(SOFT):",j)
            print(f"Frame {i} and Frame {i+1} have an intersection score below threshold: {score:.4f}")
            show_frame(frames[i], title=f"frame {i}")
            show_frame(frames[i+1], title=f"frame {i+1}")
            j+=1
          else:
            print("Scene_Cut(HARD):",k)
            print(f"Frame {i} and Frame {i+1} have an intersection score below threshold: {score:.4f}")
            show_frame(frames[i], title=f"frame {i}")
            show_frame(frames[i+1], title=f"frame {i+1}")
            k+=1


    return frames, scores

# Example usage
video_path = "/content/sample_data/scut - Made with Clipchamp.mp4"
frames, scores = extract_frames_and_compare(video_path)

# Display some frames using matplotlib
def show_frame(frame, title="Frame"):
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_HSV2RGB)  # Convert back to RGB for display
    plt.imshow(rgb_frame)
    plt.title(title)
    plt.axis('off')
    plt.show()
#_index+1], title=f"Frame with Highest Intersection Score: {max_score_index+1}")

# Identify and display the frames with scores less than the average
j=1
for i, score in enumerate(scores):
    if score < 2.2:
      print("Scene_Cut:",j)
      print(f"Frame {i} and Frame {i+1} have an intersection score below threshold: {score:.4f}")
      show_frame(frames[i], title=f"frame {i}")
      show_frame(frames[i+1], title=f"frame {i+1}")
      j+=1

